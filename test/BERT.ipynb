{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-06 19:31:57.190\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classification_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m324\u001b[0m - \u001b[34m\u001b[1mDevice: mps\u001b[0m\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-07-06 19:31:57.745\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classifier\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m112\u001b[0m - \u001b[34m\u001b[1mtrain model ...\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpytextclassifier.data_helper\u001b[0m:\u001b[36mset_seed\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mSet seed for random, numpy and torch: 1\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.base_classifier\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mloaded data list, X size: 6, y size: 6\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.base_classifier\u001b[0m:\u001b[36mload_data\u001b[0m:\u001b[36m46\u001b[0m - \u001b[34m\u001b[1mnum_classes: 2, labels: ['education', 'sports']\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classifier\u001b[0m:\u001b[36mbuild_labels_map\u001b[0m:\u001b[36m271\u001b[0m - \u001b[34m\u001b[1mlabel vocab size: 2, label_vocab_path: models/bert-chinese-toy/label_vocab.json\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.753\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classifier\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m144\u001b[0m - \u001b[34m\u001b[1mtrain_data size: 5\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classifier\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m145\u001b[0m - \u001b[34m\u001b[1mtrain_data sample:\n",
      "      labels                   text\n",
      "1  education    中国高考成绩海外认可 是“狼来了”吗？\n",
      "4     sports  四川丹棱举行全国长距登山挑战赛 近万人参与\n",
      "0  education     名师指导托福语法技巧：名词的复数形式\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.756\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classifier\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mdev_data size: 1\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:57.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classifier\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m148\u001b[0m - \u001b[34m\u001b[1mdev_data sample:\n",
      "      labels                text\n",
      "2  education  公务员考虑越来越吃香，这是怎么回事？\u001b[0m\n",
      "\u001b[32m2024-07-06 19:31:59.022\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpytextclassifier.bert_classfication_utils\u001b[0m:\u001b[36mbuild_classification_dataset\u001b[0m:\u001b[36m309\u001b[0m - \u001b[34m\u001b[1m Converting to features started. Cache is not used.\u001b[0m\n",
      "/Users/kevintan/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epochs 0/2. Running Loss:    0.5825:   0%|          | 0/1 [00:02<?, ?it/s]\n",
      "Epoch 1 of 2:   0%|          | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 6.28 GB, other allocations: 509.32 MB, max allowed: 6.77 GB). Tried to allocate 9.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# model_type: support 'bert', 'albert', 'roberta', 'xlnet'\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model_name: support 'bert-base-chinese', 'bert-base-cased', 'bert-base-multilingual-cased' ...\u001b[39;00m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m名师指导托福语法技巧：名词的复数形式\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m中国高考成绩海外认可 是“狼来了”吗？\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msports\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m米兰客场8战不败国米10年连胜\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     18\u001b[0m ]\n\u001b[0;32m---> 19\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# load trained best model from model_dir\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/pytextclassifier/bert_classifier.py:154\u001b[0m, in \u001b[0;36mBertClassifier.train\u001b[0;34m(self, data_list_or_path, dev_data_list_or_path, header, names, delimiter, test_size)\u001b[0m\n\u001b[1;32m    152\u001b[0m     dev_data \u001b[38;5;241m=\u001b[39m dev_data_list_or_path\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels_map\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_list\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_model(\n\u001b[1;32m    160\u001b[0m         train_data,\n\u001b[1;32m    161\u001b[0m         args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_map\u001b[39m\u001b[38;5;124m'\u001b[39m: labels_map, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_list\u001b[39m\u001b[38;5;124m'\u001b[39m: labels_list}\n\u001b[1;32m    162\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/pytextclassifier/bert_classification_model.py:522\u001b[0m, in \u001b[0;36mBertClassificationModel.train_model\u001b[0;34m(self, train_df, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    514\u001b[0m     train_dataset,\n\u001b[1;32m    515\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mtrain_sampler,\n\u001b[1;32m    516\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    517\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    518\u001b[0m )\n\u001b[1;32m    520\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 522\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/pytextclassifier/bert_classification_model.py:839\u001b[0m, in \u001b[0;36mBertClassificationModel.train\u001b[0;34m(self, train_dataloader, output_dir, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 839\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update learning rate schedule\u001b[39;00m\n\u001b[1;32m    841\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/iComplaint/.venv/lib/python3.9/site-packages/transformers/optimization.py:635\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    633\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    637\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 6.28 GB, other allocations: 509.32 MB, max allowed: 6.77 GB). Tried to allocate 9.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from pytextclassifier import BertClassifier\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = BertClassifier(output_dir='models/bert-chinese-toy', num_classes=2,\n",
    "                       model_type='bert', model_name='bert-base-chinese', num_epochs=2)\n",
    "    # model_type: support 'bert', 'albert', 'roberta', 'xlnet'\n",
    "    # model_name: support 'bert-base-chinese', 'bert-base-cased', 'bert-base-multilingual-cased' ...\n",
    "    data = [\n",
    "        ('education', '名师指导托福语法技巧：名词的复数形式'),\n",
    "        ('education', '中国高考成绩海外认可 是“狼来了”吗？'),\n",
    "        ('education', '公务员考虑越来越吃香，这是怎么回事？'),\n",
    "        ('sports', '图文：法网孟菲尔斯苦战进16强 孟菲尔斯怒吼'),\n",
    "        ('sports', '四川丹棱举行全国长距登山挑战赛 近万人参与'),\n",
    "        ('sports', '米兰客场8战不败国米10年连胜'),\n",
    "    ]\n",
    "    m.train(data)\n",
    "    print(m)\n",
    "    # load trained best model from model_dir\n",
    "    m.load_model()\n",
    "    predict_label, predict_proba = m.predict(['福建春季公务员考试报名18日截止 2月6日考试',\n",
    "                                              '意甲首轮补赛交战记录:米兰客场8战不败国米10年连胜'])\n",
    "    print(f'predict_label: {predict_label}, predict_proba: {predict_proba}')\n",
    "\n",
    "    test_data = [\n",
    "        ('education', '福建春季公务员考试报名18日截止 2月6日考试'),\n",
    "        ('sports', '意甲首轮补赛交战记录:米兰客场8战不败国米10年连胜'),\n",
    "    ]\n",
    "    acc_score = m.evaluate_model(test_data)\n",
    "    print(f'acc_score: {acc_score}')\n",
    "\n",
    "    # train model with 1w data file and 10 classes\n",
    "    print('-' * 42)\n",
    "    m = BertClassifier(output_dir='models/bert-chinese', num_classes=10,\n",
    "                       model_type='bert', model_name='bert-base-chinese', num_epochs=2,\n",
    "                       args={\"no_cache\": True, \"lazy_loading\": True, \"lazy_text_column\": 1, \"lazy_labels_column\": 0, })\n",
    "    data_file = 'thucnews_train_1w.txt'\n",
    "    # 如果训练数据超过百万条，建议使用lazy_loading模式，减少内存占用\n",
    "    m.train(data_file, test_size=0, names=('labels', 'text'))\n",
    "    m.load_model()\n",
    "    predict_label, predict_proba = m.predict(\n",
    "        ['顺义北京苏活88平米起精装房在售',\n",
    "         '美EB-5项目“15日快速移民”将推迟',\n",
    "         '恒生AH溢指收平 A股对H股折价1.95%'])\n",
    "    print(f'predict_label: {predict_label}, predict_proba: {predict_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
